{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Is park weather associated with visitor counts for 2022?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "Null:  Weather and visitation are not correlated.\n",
    "\n",
    "Alternate:  Parks with average summer temperatures between 70-80 degrees have more visitors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "from configure import VC_API_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pull visitor and park info from resources, especially looking for lat and long and at the weather column.\n",
    "\n",
    "# Dan's code\n",
    "from configure import NPS_API_key\n",
    "from pathlib import Path\n",
    "import re\n",
    "from pandas import json_normalize \n",
    "\n",
    "park_visitors_csv_file = Path(\"../Resources/national_park_visitors.csv\")\n",
    "\n",
    "#Read the csv and create variable called park_visitors_data\n",
    "park_visitors_data= pd.read_csv(park_visitors_csv_file)\n",
    "\n",
    "#Make it into a dataframe\n",
    "park_visitors_df = pd.DataFrame(park_visitors_data)\n",
    "\n",
    "#Establish nps endpoint - with api key\n",
    "endpoint = f\"https://developer.nps.gov/api/v1/parks?limit=600&api_key={NPS_API_key}\"\n",
    "\n",
    "#turn the result into a json file\n",
    "parks_data = requests.get(endpoint).json()\n",
    "\n",
    "#Normalize data so it all fits into a dataframe\n",
    "normalized_parks_data = json_normalize(parks_data, 'data')\n",
    "\n",
    "#Drop the columns we dont need\n",
    "all_parks_df = pd.DataFrame(normalized_parks_data).drop(columns=[\n",
    "'id', \n",
    "    'url', \n",
    "    'directionsInfo', \n",
    "    'directionsUrl', \n",
    "    'addresses',\n",
    "    'latLong',\n",
    "    'images', \n",
    "    'contacts.phoneNumbers',\n",
    "    'contacts.emailAddresses',\n",
    "    \n",
    "])\n",
    "\n",
    "#sort by only those named national park or national park and preserve\n",
    "national_parks_df = all_parks_df.loc[(all_parks_df['designation'] == \"National Park\") | (all_parks_df['designation'] == \"National Park & Preserve\")]\n",
    "\n",
    "#Clean up the \" NP\" Part of the Park Name\n",
    "park_visitors_df_NP_clean = park_visitors_df.replace(' NP','', regex=True)\n",
    "\n",
    "#Clean up the \"& PRES\" Part of the Park Name\n",
    "park_visitors_df_PRES_clean = park_visitors_df_NP_clean.replace(' & PRES','', regex=True)\n",
    "\n",
    "#Make Park Name match for those that do not\n",
    "park_visitors_df_clean = park_visitors_df_PRES_clean.replace('Hawaii Volcanoes','Hawaiʻi Volcanoes', regex=True)\n",
    "park_visitors_df_clean = park_visitors_df_clean.replace('Haleakala','Haleakalā', regex=True)\n",
    "park_visitors_df_clean = park_visitors_df_clean.replace('Black Canyon of the Gunnison','Black Canyon Of The Gunnison', regex=True)\n",
    "park_visitors_df_clean = park_visitors_df_clean.replace('Wrangell-St. Elias','Wrangell - St Elias', regex=True)\n",
    "park_visitors_df_clean = park_visitors_df_clean.replace('Gates of the Arctic','Gates Of The Arctic', regex=True)\n",
    "\n",
    "#rename park_visitors_df columns\n",
    "park_visitors_df_clean.columns = [\"Park Name\", \"Rank\", \"Visitors\", \"Percent Of Total\"]\n",
    "\n",
    "#create a copy to start renaming columns\n",
    "national_parks_df_clean = national_parks_df.copy()\n",
    "\n",
    "#rename columns\n",
    "national_parks_df_clean.rename(columns={\n",
    "    'fullName':'Full Name',\n",
    "    'parkCode':'Park Code',\n",
    "    'description':'Description',\n",
    "    'latitude':'Latitude',\n",
    "    'longitude':'Longitude',\n",
    "    'activities':'Activities',\n",
    "    'topics':'Topics',\n",
    "    'states':'States',\n",
    "    'entranceFees':'Entrance Fees',\n",
    "    'entrancePasses':'Entrance Passes',\n",
    "    'fees':'Fees',\n",
    "    'operatingHours':'Operating Hours',\n",
    "    'weatherInfo':'Weather Info',\n",
    "    'designation':'Designation',\n",
    "    'name':'Park Name'\n",
    "}, inplace = True)\n",
    "\n",
    "#merge visitor df and park df\n",
    "combined_park_data_df = park_visitors_df_clean.merge(national_parks_df_clean, on=\"Park Name\", how='left')\n",
    "\n",
    "print(len(combined_park_data_df))\n",
    "combined_park_data_df\n",
    "\n",
    "#end Dan's code (modified for weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_prep_data = combined_park_data_df[[\"Park Name\", \"Rank\", \"Visitors\", \"Percent Of Total\", \"Full Name\", \"Latitude\", \"Longitude\", \"Weather Info\"]]\n",
    "weather_data_prepped = weather_prep_data.copy()\n",
    "weather_data_prepped[\"Lat/Lon\"] = weather_prep_data[\"Latitude\"].str.cat(weather_prep_data[\"Longitude\"], sep = \", \")\n",
    "weather_data_prepped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API to Visual Crossing for weather data from summer of 2022\n",
    "import urllib.parse\n",
    "\n",
    "# Create list of results for all dates for each location\n",
    "results = []\n",
    "\n",
    "dates = [\"2022-05-01\",\n",
    "        \"2022-05-15\",\n",
    "        \"2022-05-29\",\n",
    "        \"2022-06-12\",\n",
    "        \"2022-06-26\",\n",
    "        \"2022-07-10\",\n",
    "        \"2022-07-24\",\n",
    "        \"2022-08-07\",\n",
    "        \"2022-08-21\",\n",
    "        \"2022-09-04\",\n",
    "        \"2022-09-18\"]\n",
    "\n",
    "# set up a parameters dictionary\n",
    "params = {\n",
    "    \"unitGroup\":\"us\",\n",
    "    \"elements\":\"datetime,name,tempmax,tempmin,precip,cloudcover\",\n",
    "    \"include\":\"days,obs\",\n",
    "    \"key\":VC_API_key,\n",
    "    \"contentType\":\"json\"    \n",
    "}\n",
    "for park, row in weather_data_prepped.iterrows():\n",
    "    lat = row[\"Latitude\"]\n",
    "    lon = row[\"Longitude\"]\n",
    "    for date in dates:\n",
    "        # Set base URL\n",
    "        # This url works:  url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/35.60116374%2C%20-83.50818326/2022-05-01/2022-05-01?unitGroup=us&elements=datetime%2Cname%2Ctempmax%2Ctempmin%2Cprecip%2Ccloudcover&include=days%2Cobs&key={VC_API_key}&contentType=json\"\n",
    "        base_url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{lat}%2C%20{lon}/{date}/{date}\"\n",
    "        query_string = urllib.parse.urlencode(params)  #from GPTChat\n",
    "        \n",
    "        # Build the URL using an f-string and get data\n",
    "        url = f'{base_url}?{query_string}'\n",
    "        # print(url)\n",
    "        park_weather_data = requests.get(url).json()\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            park_weather_data = response.json()\n",
    "            results.append(park_weather_data)\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        results.append(park_weather_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all the weather data\n",
    "\n",
    "# Initialize empty lists for storing tempmin, tempmax, precip, and cloudcover values\n",
    "lat_lon_ls = []\n",
    "date_ls = []\n",
    "tempmin_ls = []\n",
    "tempmax_ls = []\n",
    "precip_ls = []\n",
    "cloudcover_ls = []\n",
    "\n",
    "# Parse JSON and add our 4 values to their respective lists\n",
    "for json_data in results:\n",
    "    days_data = json_data[\"days\"]\n",
    "    if days_data:\n",
    "        #Add lat,lon for eventual merge with park data.\n",
    "        weather_lat = json_data[\"latitude\"]\n",
    "        weather_lon = json_data[\"longitude\"]\n",
    "        weather_lat_lon = f\"{weather_lat}, {weather_lon}\"\n",
    "        lat_lon_ls.append(weather_lat_lon)\n",
    "        day = days_data[0]\n",
    "        obs_date = day[\"datetime\"]\n",
    "        date_ls.append(obs_date)\n",
    "        tempmax = day[\"tempmax\"]\n",
    "        tempmax_ls.append(tempmax)\n",
    "        tempmin = day[\"tempmin\"]\n",
    "        tempmin_ls.append(tempmin)\n",
    "        precip = day[\"precip\"]\n",
    "        precip_ls.append(precip)\n",
    "        cloudcover = day[\"cloudcover\"]\n",
    "        cloudcover_ls.append(cloudcover)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "weather_df = pd.DataFrame({\n",
    "    \"Lat/Lon\" : lat_lon_ls,\n",
    "    \"Date\": date_ls,\n",
    "    \"Temp Min\": tempmin_ls,\n",
    "    \"Temp Max\": tempmax_ls,\n",
    "    \"Precipitation\": precip_ls,\n",
    "    \"Cloud Cover\": cloudcover_ls\n",
    "})\n",
    "\n",
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average summer conditions for each park\n",
    "avg_tempmin = weather_df.groupby(\"Lat/Lon\") [\"Temp Min\"].mean()\n",
    "avg_tempmax = weather_df.groupby(\"Lat/Lon\") [\"Temp Max\"].mean()\n",
    "avg_precip = weather_df.groupby(\"Lat/Lon\") [\"Precipitation\"].mean()\n",
    "avg_cloudcover = weather_df.groupby(\"Lat/Lon\") [\"Cloud Cover\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with the park averages\n",
    "park_avgs = pd.DataFrame([avg_tempmin, avg_tempmax, avg_precip, avg_cloudcover])\n",
    "park_avgs = park_avgs.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge park_avgs dataframe with weather_data on lat,lon and show\n",
    "final_weather_data = weather_data_prepped.merge(park_avgs, how = \"left\", on = \"Lat/Lon\")\n",
    "final_weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop parks with missing temp data\n",
    "# At lat/Lon:  \n",
    "# 42.94065854, -122.1338414 = Crater Lake\n",
    "# 61.4182147, -142.6028439 = Wrangell - St Elias\n",
    "# And our known missing parks:  Sequoia, Kings Canyon, Redwood, and National Park of American Samoa\n",
    "\n",
    "weather_minus_zeros = final_weather_data[final_weather_data[\"Temp Max\"]!=0]\n",
    "clean_weather_data = weather_minus_zeros.dropna(subset=['Temp Max'])\n",
    "clean_weather_data.info()  #checking data type for visitors and how many NaN values are in Preciptation and Cloud Cover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot number of visitors vs avg temp max, pearson's r if it looks linear\n",
    "\n",
    "visitors_by_thousand = clean_weather_data.copy()\n",
    "visitors_by_thousand[\"Visitors\"] = visitors_by_thousand[\"Visitors\"].str.replace(\",\", \"\").astype(int)\n",
    "visitors_by_thousand[\"Visitors (k)\"] = visitors_by_thousand[\"Visitors\"] / 1000\n",
    "plt.scatter(visitors_by_thousand[\"Visitors (k)\"], visitors_by_thousand[\"Temp Max\"])\n",
    "plt.title(\"NP Visitors by Avg Park Temperature\")\n",
    "plt.xlabel(\"Visitors in thousands\")\n",
    "plt.ylabel(\"Temperature (F)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression between Avg Max Temp and Num of Visitors by Park\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "visitors = visitors_by_thousand[\"Visitors (k)\"]\n",
    "max_temp = visitors_by_thousand[\"Temp Max\"]\n",
    "print(f\"The correlation coefficient between number of visitors and average maximum temperature is {round(st.pearsonr(visitors,max_temp)[0],2)}.\")\n",
    "\n",
    "pe_slope, pe_int, pe_r, pe_p, pe_std_err  = st.linregress(visitors, max_temp)\n",
    "pe_fit = pe_slope * visitors + pe_int\n",
    "\n",
    "plt.scatter(visitors_by_thousand[\"Visitors (k)\"], visitors_by_thousand[\"Temp Max\"])\n",
    "plt.plot(visitors,pe_fit,\"--\",color=\"r\")\n",
    "plt.title(\"NP Visitors by Avg Park Temperature\")\n",
    "plt.xlabel(\"Visitors in thousands\")\n",
    "plt.ylabel(\"Temperature (F)\")\n",
    "\n",
    "plt.savefig(\"Images/Avg_Temp_Scatter.png\") #dpi:  Higher number increases quality of picture AND size of file.  100 is default, 200 is double. Transparent applies to whether the background is clear or white.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression between Avg Min Temp and Num of Visitors by Park\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "visitors = visitors_by_thousand[\"Visitors (k)\"]\n",
    "min_temp = visitors_by_thousand[\"Temp Min\"]\n",
    "print(f\"The correlation coefficient between number of visitors and average minimum temperature is {round(st.pearsonr(visitors,min_temp)[0],2)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression between Avg Precipitation and Num of Visitors by Park\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "precip_df = visitors_by_thousand.dropna(subset = [\"Precipitation\"])\n",
    "visitors = precip_df[\"Visitors (k)\"]\n",
    "precip = precip_df[\"Precipitation\"]\n",
    "print(f\"The correlation coefficient between number of visitors and average precipitation is {round(st.pearsonr(visitors,precip)[0],2)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression between Avg Cloud Cover and Num of Visitors by Park\n",
    "\n",
    "# Calculate the correlation coefficient\n",
    "cloudcover_df = visitors_by_thousand.dropna(subset = [\"Cloud Cover\"])\n",
    "visitors = cloudcover_df[\"Visitors (k)\"]\n",
    "cloudcover = cloudcover_df[\"Cloud Cover\"]\n",
    "print(f\"The correlation coefficient between number of visitors and average cloud cover is {round(st.pearsonr(visitors,cloudcover)[0],2)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put parks in bins by avg maxtemp ranges\n",
    "park_bins = clean_weather_data.copy()\n",
    "park_bins[\"Visitors\"] = park_bins[\"Visitors\"].str.replace(\",\", \"\").astype(int)\n",
    "park_bins[\"Visitors (k)\"] = park_bins[\"Visitors\"] / 1000\n",
    "bins = [0, 60, 70, 80, 90, 100]\n",
    "labels = ['<60 degrees', '60-70 degrees', '70-80 degrees', '80-90 degrees', '90-100 degrees']\n",
    "park_bins['Avg Temp Range'] = pd.cut(x = park_bins['Temp Max'], bins = bins, labels = labels, include_lowest = True)\n",
    "park_bins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are more high visitor parks in any bin?\n",
    "# Create a box plot with temperature range bins.\n",
    "\n",
    "temp_ranges = ['<60 degrees', '60-70 degrees', '70-80 degrees', '80-90 degrees', '90-100 degrees']\n",
    "\n",
    "# Create empty list to fill with grouped visitor data\n",
    "visitors_by_range = []\n",
    "\n",
    "# Create list of visitors for each temperature range.\n",
    "for temp in temp_ranges: \n",
    "    range_group = park_bins.loc[park_bins[\"Avg Temp Range\"] == temp, \"Visitors (k)\"]\n",
    "    visitors_by_range.append(range_group)\n",
    "    quartiles = range_group.quantile([.25,.5,.75])\n",
    "    lowerq = quartiles[0.25]\n",
    "    upperq = quartiles[0.75]\n",
    "    iqr = upperq-lowerq\n",
    "    lower_bound = lowerq - (1.5*iqr)\n",
    "    upper_bound = upperq + (1.5*iqr)\n",
    "    \n",
    "plt.boxplot(visitors_by_range, labels = temp_ranges)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title(\"Visitors to Parks by Temperature Ranges\")\n",
    "plt.xlabel(\"Average Temperature Range (F)\")\n",
    "plt.ylabel(\"Annual Visitors (k)\")\n",
    "\n",
    "plt.savefig(\"Images/Temperature_Range_Box_Plot.png\", bbox_inches='tight') #dpi:  Higher number increases quality of picture AND size of file.  100 is default, 200 is double. Transparent applies to whether the background is clear or white.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Testing\n",
    "# GPTChat suggested that the best statistical test for numerical data in 5 categories is the Kruskal-Wallis test rather than any we learned in class.\n",
    "# The next code is how ChatGPT recommended splitting visitors_by_range (the list of series of visitors by temperature range bins) into 5 separate lists to use for the test.\n",
    "\n",
    "import string\n",
    "\n",
    "series_list = visitors_by_range\n",
    "\n",
    "group_dict = {}\n",
    "groups = string.ascii_uppercase[:len(series_list)]  # A, B, C, D, E\n",
    "\n",
    "for group, series in zip(groups, series_list):   \n",
    "    group_dict[group] = series.tolist()\n",
    "\n",
    "group_A = group_dict['A']\n",
    "group_B = group_dict['B']\n",
    "group_C = group_dict['C']\n",
    "group_D = group_dict['D']\n",
    "group_E = group_dict['E']\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "statistic, p_value = st.kruskal(group_A, group_B, group_C, group_D, group_E)\n",
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
